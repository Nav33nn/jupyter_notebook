{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment 2A2B.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"wjUSI6-TWeyf","colab_type":"text"},"cell_type":"markdown","source":["# **Neural networks explained**"]},{"metadata":{"id":"SlUKlivWawvD","colab_type":"text"},"cell_type":"markdown","source":["\n","Backpropagation is a commonly used technique for training neural network. There are many resources explaining the technique, but this post will explain backpropagation with concrete examples.\n","\n","**Overview**\n","\n","\n","In this post, we will build a neural network with three layers:\n","\n","\n","\n","*   Input layer with two inputs neurons\n","*   One hidden layer with two neurons\n","*   Output layer with a single neuron\n"]},{"metadata":{"id":"LSuS_ThjbZai","colab_type":"text"},"cell_type":"markdown","source":["***Note: The approach I've taken here is that after every block of explanation, there will be a code in python that does the exact same operation. ***"]},{"metadata":{"id":"ltc9Zv7_bqUB","colab_type":"text"},"cell_type":"markdown","source":["Lets consider a network shown below\n","\n","![alt text](http://hmkcode.github.io/images/ai/nn1.png)"]},{"metadata":{"id":"jNmWws1IcTnk","colab_type":"text"},"cell_type":"markdown","source":["Neural network training is about finding weights that minimize prediction error. We usually start our training with a set of randomly generated weights.Then, backpropagation is used to update the weights in an attempt to correctly map arbitrary inputs to outputs.\n","\n","For our example, let's consider our initial weights to be\n","\n","\n","**w1** = 0.34,  **w2**= 0.13,  **w3** = 0.05,  **w4** = 0.07,  **w5** = 0.20 and **w6** = 0.01"]},{"metadata":{"id":"2tzZwivScT4N","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","Let's say that out inputs are\n","\n","$i_{1}$ = 2 and $i_{2}$  = 3\n","\n","and our output \n","\n","out = 1\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"M15DwY1XtKrN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"a0727684-9a3e-416a-b7a8-5eb82ecdfcdc","executionInfo":{"status":"ok","timestamp":1551788084326,"user_tz":-330,"elapsed":925,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}}},"cell_type":"code","source":["'''\n","We will be using numpy for our matrix operations. Let's import numpy here and initialize the weight matrices\n","'''\n","\n","import numpy as np\n","\n","inputs = np.array([2,3])\n","f_weights = np.array([[0.34,0.13],[0.05,0.07]]).T#weights of the first layer\n","s_weights = np.array([0.2,0.01])\n","expected_output = np.array([1])\n","\n","print('first matrix', f_weights)\n","print('second_matrix', s_weights)\n","print('inputs',inputs)\n","print('expected output', expected_output)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["first matrix [[0.34 0.05]\n"," [0.13 0.07]]\n","second_matrix [0.2  0.01]\n","inputs [2 3]\n","expected output [1]\n"],"name":"stdout"}]},{"metadata":{"id":"OWhW_2K8cT10","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---"]},{"metadata":{"id":"tKPNigGFcTzl","colab_type":"text"},"cell_type":"markdown","source":["##Dataset\n","\n","So, out input = [2,3] and output = [1]"]},{"metadata":{"id":"RcRsOBFpcTw-","colab_type":"text"},"cell_type":"markdown","source":["One Epoch in a training consists of the following\n","\n","\n","*   Forward Pass\n","*   Calculating Error\n","*   Reducing Error\n","*   Backpropogation\n","*   Backward Pass\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"lczH-i0xcTus","colab_type":"text"},"cell_type":"markdown","source":["##Forward Pass\n","\n","We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then passed forward to next layer.\n","\n","$\\begin{bmatrix}2 & 3\\end{bmatrix}$ . $\\begin{bmatrix}0.34 & 0.05\\\\0.13 & 0.07\\end{bmatrix}$\n"," . $\\begin{bmatrix}0.20\\\\0.01\\end{bmatrix}$\n"," \n","**h1** = 2 x 0.34 + 2 x 0.13 = 0.94\n","\n","**h2** = 3 x 0.05 + 3 x 0.07 = 0.29\n","\n"," $\\begin{bmatrix}0.94 &0.29\\end{bmatrix}$ . $\\begin{bmatrix}0.20\\\\0.01\\end{bmatrix}$ \n"," \n"," (0.94 x 0.20) + (0.29 x 0.01) = 0.190\n"," \n"," \n"," Hence the new matrix is   $\\begin{bmatrix}0.190\\end{bmatrix}$\n","\n"]},{"metadata":{"id":"5j93eXvotHTL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"8f111942-1e36-4146-ebc5-4012b4eb87ce","executionInfo":{"status":"ok","timestamp":1551788089423,"user_tz":-330,"elapsed":1229,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}}},"cell_type":"code","source":["'''\n","Let's compute the values on the hidden nodes\n","'''\n","h_weights = np.dot(inputs, f_weights)\n","print('hidden weights',h_weights)\n","\n","output = np.dot(h_weights, s_weights)\n","print('output ',output)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["hidden weights [1.07 0.31]\n","output  0.21710000000000002\n"],"name":"stdout"}]},{"metadata":{"id":"JAUtkswUcTsk","colab_type":"text"},"cell_type":"markdown","source":["##Calculating Error\n","\n","Now, it’s time to find out how our network performed by calculating the difference between the actual output and predicted one. It’s clear that our network output, or prediction, is not even close to actual output. We can calculate the difference or the error as following.\n","\n","\n","$$Error = \\frac{1}{2}(prediction - actual)^{2}$$\n","\n","\n","Hence,\n","\n","$$Error = \\frac{1}{2}(0.190 - 1.0)^{2} = 0.32805$$\n","\n","\n","---\n","\n"]},{"metadata":{"id":"guZM6HttxPrD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"533c70c6-79bb-4c9e-c8f6-bcb335285e4a","executionInfo":{"status":"ok","timestamp":1551788168588,"user_tz":-330,"elapsed":899,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}}},"cell_type":"code","source":["'''\n","Let's calculate the error\n","'''\n","error = 0.5*((output - expected_output[0])**2)\n","print('error ', error)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["error  0.30646620499999994\n"],"name":"stdout"}]},{"metadata":{"id":"OPXWYnqEcTqn","colab_type":"text"},"cell_type":"markdown","source":["##Reducing Error\n","\n","Our main goal of the training is to reduce the error or the difference between prediction and actual output. Since actual output is constant, “not changing”, the only way to reduce the error is to change prediction value. The question now is, how to change prediction value?\n","\n","By decomposing prediction into its basic elements we can find that weights are the variable elements affecting prediction value. In other words, in order to change prediction value, we need to change weights values.\n","\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_prediction_elements.png)\n","\n","\n","The question now is how to change\\update the weights value so that the error is reduced?\n","The answer is Backpropagation!\n","\n","\n","\n","---\n","\n"]},{"metadata":{"id":"1FoSGyvqcTlR","colab_type":"text"},"cell_type":"markdown","source":["##Backpropogation\n","\n","Backpropagation, short for “backward propagation of errors”, is a mechanism used to update the weights using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). It calculates the gradient of the error function with respect to the neural network’s weights. The calculation proceeds backwards through the network.\n","\n","Gradient descent is an iterative optimization algorithm for finding the minimum of a function; in our case we want to minimize th error function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point.\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_update_formula.png)\n","\n","For example, to update **w6**, we take the current **w6** and subtract the partial derivative of error function with respect to **w6**. Optionally, we multiply the derivative of the error function by a selected number to make sure that the new updated weight is minimizing the error function; this number is called learning rate.\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_w6_update.png)\n","\n","The derivation of the error function is evaluated by applying the chain rule as following\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w6.png)\n","\n","So to update **w6** we can apply the following formula\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_w6_update_closed_form.png)\n","\n","Similarly, we can derive the update formula for **w5** and any other weights existing between the output and the hidden layer.\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_w5_update_closed_form.png)\n","\n","However, when moving backward to update **w1**, **w2**, **w3** and **w4** existing between input and hidden layer, the partial derivative for the error function with respect to **w1**, for example, will be as following.\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_error_function_partial_derivative_w1.png)\n","\n","We can find the update formula for the remaining weights **w2**, **w3** and **w4** in the same way.\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_update_all_weights.png)\n","\n","\n","We can rewrite the update formulas in matrices as following\n","\n","![alt text](http://hmkcode.github.io/images/ai/bp_update_all_weights_matrix.png)"]},{"metadata":{"id":"wYlHE-cyAbWQ","colab_type":"text"},"cell_type":"markdown","source":["##Backward Pass\n","\n","\n","Using derived formulas we can find the new weights.\n","\n","Learning rate: is a hyperparameter which means that we need to manually guess its value.\n","\n","$\\Delta = 0.190 - 1  =  -0.81$           \n","\n","Delta = Predicted - Actual\n","\n","a = 0.05 (Learning Rate)\n","\n","\n","now, \n","\n","**w1** = 0.34,  **w2**= 0.13,  **w3** = 0.05,  **w4** = 0.07,  **w5** = 0.20, **w6** = 0.01, **h1** = 0.94 and **h2** = 0.29\n","\n","\n"," $\\begin{bmatrix}w5\\\\w6\\end{bmatrix} =  \\begin{bmatrix}0.20\\\\0.01\\end{bmatrix} - 0.05(-0.81) \\begin{bmatrix}0.94\\\\0.29\\end{bmatrix} = \\begin{bmatrix}0.20\\\\0.01\\end{bmatrix} - \\begin{bmatrix}-0.038\\\\-0.011\\end{bmatrix} = \\begin{bmatrix}0.238\\\\0.021\\end{bmatrix}$ \n","\n","\n","$\\begin{bmatrix}w1 & w3\\\\w2 & w4\\end{bmatrix} = \\begin{bmatrix}0.34 & 0.05\\\\0.13 & 0.07\\end{bmatrix} - 0.05(-0.81) \\begin{bmatrix}2\\\\3\\end{bmatrix} .   \\begin{bmatrix}0.20 & 0.01\\end{bmatrix}= \\begin{bmatrix}0.34 & 0.05\\\\0.13 & 0.07\\end{bmatrix} - \\begin{bmatrix}-0.016 & 0\\\\ -0.024&-0.01 \\end{bmatrix} =   \\begin{bmatrix}0.356 &0.05\\\\ 0.154&0.08 \\end{bmatrix}$\n","\n"]},{"metadata":{"id":"LMkVXHcJMrVT","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","**Now, using the new weights we will repeat the forward passed**\n","\n","\n","\n","$\\begin{bmatrix}2 & 3\\end{bmatrix}$ . $\\begin{bmatrix}0.356 & 0.05\\\\0.154 & 0.08\\end{bmatrix}$\n"," . $\\begin{bmatrix}0.238\\\\0.021\\end{bmatrix}$\n"," \n","**h1** = 2 x 0.356 + 2 x 0.154 = 1.02\n","\n","**h2** = 3 x 0.05 + 3 x 0.08 = 0.39\n","\n"," $\\begin{bmatrix}1.02 &0.39\\end{bmatrix}$ . $\\begin{bmatrix}0.238\\\\0.021\\end{bmatrix}$ \n"," \n"," (1.02x 0.238) + (0.39 x 0.021) = 0.251\n"," \n"," \n"," Hence the new output is   $\\begin{bmatrix}0.251\\end{bmatrix}$\n"," \n"," \n"," \n"," \n"," \n"," **We can notice that the prediction 0.251 is a little bit closer to actual output than the previously predicted one 0.190. We can repeat the same process of backward and forward pass until error is close or equal to zero.**"]},{"metadata":{"id":"yk9RPbzu0LJo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"outputId":"4b8169c2-5be3-4b50-dd61-2f713a1e1f13","executionInfo":{"status":"ok","timestamp":1551789806178,"user_tz":-330,"elapsed":962,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}}},"cell_type":"code","source":["'''\n","Let's do the backward pass here\n","'''\n","print('\\nfirst layer weights -->', f_weights)\n","print('\\nsecond layer weights -->', s_weights)\n","print('\\nhidden layer values -->', h_weights)\n","\n","delta = output - expected_output\n","print('\\ndelta -->', delta)\n","\n","lr = 0.05\n","print('\\nlearning rate --> ', lr)\n","\n","updated_second_layer_weights = s_weights - lr*delta*h_weights\n","print('\\non errror corrections')\n","print('\\nUpdated second layer weights are',updated_second_layer_weights)\n","updated_first_layer_weights = f_weights - lr*delta*np.dot(inputs,s_weights)\n","print('\\nUpdated first layer weights are',updated_first_layer_weights)\n","\n","\n","'''\n","Let's use the new weigths to perform a forward pass now\n","\n","Let's compute the values on the hidden nodes\n","'''\n","updated_h_weights = np.dot(inputs, updated_first_layer_weights)\n","print('\\nUpdated hidden weights',updated_h_weights)\n","\n","new_output = np.dot(updated_h_weights, updated_second_layer_weights)\n","print('\\nUpdated output ',new_output)\n","\n","'''\n","Let's calculate the error\n","'''\n","new_error = 0.5*((new_output - expected_output[0])**2)\n","print('\\nUpdated error ', new_error)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["\n","first layer weights --> [[0.34 0.05]\n"," [0.13 0.07]]\n","\n","second layer weights --> [0.2  0.01]\n","\n","hidden layer values --> [1.07 0.31]\n","\n","delta --> [-0.7829]\n","\n","learning rate -->  0.05\n","\n","on errror corrections\n","\n","Updated second layer weights are [0.24188515 0.02213495]\n","\n","Updated first layer weights are [[0.35683235 0.06683235]\n"," [0.14683235 0.08683235]]\n","\n","Updated hidden weights [1.15416175 0.39416175]\n","\n","Updated output  0.28789933865117506\n","\n","Updated error  0.25354367594671695\n"],"name":"stdout"}]},{"metadata":{"id":"MESywRfe4bru","colab_type":"text"},"cell_type":"markdown","source":["##**Learnings**\n","\n","By the end of the post you've noticed that the weights have changed, the values of the hidden nodes have been updated, the error has reduced and the ouput is progressing towards the actual output. Doing this process continuosly will result in an error that's either equal to or close to 0. This is when you say that training has been completed on your data.\n","\n","**PS:** The matrix values will vary a bit since the calculations on the markdown are limited to 2 decimal places and numpy uses a 64 bit floating point. The values that the code produces are more precise.\n","\n","Thanks for reading"]},{"metadata":{"id":"OSMX_wYmMdtw","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}