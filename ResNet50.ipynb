{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet50.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"iB_Xb9Ha6ZOA","colab_type":"code","outputId":"78a38be7-2244-468b-e90f-637affedadfd","executionInfo":{"status":"ok","timestamp":1554034092381,"user_tz":-330,"elapsed":54007,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":2050574,"output_embedded_package_id":"16jmdOEiqrujRxKUgQz7HSqJq4Qqk_GnG"}},"cell_type":"code","source":["#download the dataset\n","!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n","#unzip the dataset\n","!unzip tiny-imagenet-200.zip"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"metadata":{"id":"kGzPPGJIp0YM","colab_type":"code","outputId":"b184115a-9dfb-46d2-8cc5-11d0521417cd","executionInfo":{"status":"ok","timestamp":1554034605720,"user_tz":-330,"elapsed":1426,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["#prepare val data classes to file relation\n","import pandas as pd\n","val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n","val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n","val_data.head()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>File</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>val_0.JPEG</td>\n","      <td>n03444034</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>val_1.JPEG</td>\n","      <td>n04067472</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>val_2.JPEG</td>\n","      <td>n04070727</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>val_3.JPEG</td>\n","      <td>n02808440</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>val_4.JPEG</td>\n","      <td>n02808440</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         File      Class\n","0  val_0.JPEG  n03444034\n","1  val_1.JPEG  n04067472\n","2  val_2.JPEG  n04070727\n","3  val_3.JPEG  n02808440\n","4  val_4.JPEG  n02808440"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"ttnryvxSswql","colab_type":"code","outputId":"aeb0a1ff-541d-45e0-8ea7-64ceb6dcf2d4","executionInfo":{"status":"ok","timestamp":1554034624778,"user_tz":-330,"elapsed":7395,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!ls"],"execution_count":14,"outputs":[{"output_type":"stream","text":["sample_data  tiny-imagenet-200\ttiny-imagenet-200.zip\n"],"name":"stdout"}]},{"metadata":{"id":"yCaCSnPKpLCJ","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use Augmentaion parameters as required.\n","from keras.preprocessing.image import ImageDataGenerator\n","train_datagen = ImageDataGenerator(\n","    rescale= 1./255,\n","    zoom_range = 0.2,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    rotation_range=60,\n","    horizontal_flip=True,\n","    shear_range=0.2,\n","    fill_mode='nearest'\n","    )\n","\n","valid_datagen = ImageDataGenerator(rescale=1./255)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h9EfkYPrqEtS","colab_type":"code","outputId":"cd4c8fef-8a93-4026-b693-5d6c3b98cf99","executionInfo":{"status":"ok","timestamp":1554034630135,"user_tz":-330,"elapsed":10285,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["#initialize train and val data generator\n","train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64, 64), color_mode='rgb', \n","                                                    batch_size=500, class_mode='categorical', shuffle=True, seed=42)\n","validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64, 64),\n","                                                    color_mode='rgb', class_mode='categorical', batch_size=200, shuffle=True, seed=42)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Found 100000 images belonging to 200 classes.\n","Found 10000 images belonging to 200 classes.\n"],"name":"stdout"}]},{"metadata":{"id":"J_AwRmTJwOyG","colab_type":"code","colab":{}},"cell_type":"code","source":["#visualize images from the dataset\n","def visualize_image(path):\n","  %matplotlib inline\n","  %pylab inline\n","  import matplotlib.pyplot as plt\n","  import matplotlib.image as mpimg\n","  img=mpimg.imread(path)\n","  imgplot = plt.imshow(img)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nbg_KdzYHRSE","colab_type":"code","outputId":"4dcad590-971c-4948-ad91-97a82eb02429","executionInfo":{"status":"ok","timestamp":1554035750900,"user_tz":-330,"elapsed":11174,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":6290}},"cell_type":"code","source":["from __future__ import division\n","\n","import six\n","from keras.models import Model\n","from keras.layers import (\n","    Input,\n","    Activation,\n","    Dense,\n","    Flatten\n",")\n","from keras.layers.convolutional import (\n","    Conv2D,\n","    MaxPooling2D,\n","    AveragePooling2D\n",")\n","from keras.layers.merge import add\n","from keras.layers.normalization import BatchNormalization\n","from keras.regularizers import l2\n","from keras import backend as K\n","\n","\n","def _bn_relu(input):\n","    \"\"\"Helper to build a BN -> relu block\n","    \"\"\"\n","    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n","    return Activation(\"relu\")(norm)\n","\n","\n","def _conv_bn_relu(**conv_params):\n","    \"\"\"Helper to build a conv -> BN -> relu block\n","    \"\"\"\n","    filters = conv_params[\"filters\"]\n","    kernel_size = conv_params[\"kernel_size\"]\n","    strides = conv_params.setdefault(\"strides\", (1, 1))\n","    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n","    padding = conv_params.setdefault(\"padding\", \"same\")\n","    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n","\n","    def f(input):\n","        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n","                      strides=strides, padding=padding,\n","                      kernel_initializer=kernel_initializer,\n","                      kernel_regularizer=kernel_regularizer)(input)\n","        return _bn_relu(conv)\n","\n","    return f\n","\n","\n","def _bn_relu_conv(**conv_params):\n","    \"\"\"Helper to build a BN -> relu -> conv block.\n","    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n","    \"\"\"\n","    filters = conv_params[\"filters\"]\n","    kernel_size = conv_params[\"kernel_size\"]\n","    strides = conv_params.setdefault(\"strides\", (1, 1))\n","    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n","    padding = conv_params.setdefault(\"padding\", \"same\")\n","    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n","\n","    def f(input):\n","        activation = _bn_relu(input)\n","        return Conv2D(filters=filters, kernel_size=kernel_size,\n","                      strides=strides, padding=padding,\n","                      kernel_initializer=kernel_initializer,\n","                      kernel_regularizer=kernel_regularizer)(activation)\n","\n","    return f\n","\n","\n","def _shortcut(input, residual):\n","    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n","    \"\"\"\n","    # Expand channels of shortcut to match residual.\n","    # Stride appropriately to match residual (width, height)\n","    # Should be int if network architecture is correctly configured.\n","    input_shape = K.int_shape(input)\n","    residual_shape = K.int_shape(residual)\n","    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n","    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n","    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n","\n","    shortcut = input\n","    # 1 X 1 conv if shape is different. Else identity.\n","    if stride_width > 1 or stride_height > 1 or not equal_channels:\n","        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n","                          kernel_size=(1, 1),\n","                          strides=(stride_width, stride_height),\n","                          padding=\"valid\",\n","                          kernel_initializer=\"he_normal\",\n","                          kernel_regularizer=l2(0.0001))(input)\n","\n","    return add([shortcut, residual])\n","\n","\n","def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n","    \"\"\"Builds a residual block with repeating bottleneck blocks.\n","    \"\"\"\n","    def f(input):\n","        for i in range(repetitions):\n","            init_strides = (1, 1)\n","            if i == 0 and not is_first_layer:\n","                init_strides = (2, 2)\n","            input = block_function(filters=filters, init_strides=init_strides,\n","                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n","        return input\n","\n","    return f\n","\n","\n","def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n","    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n","    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n","    \"\"\"\n","    def f(input):\n","\n","        if is_first_block_of_first_layer:\n","            # don't repeat bn->relu since we just did bn->relu->maxpool\n","            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n","                           strides=init_strides,\n","                           padding=\"same\",\n","                           kernel_initializer=\"he_normal\",\n","                           kernel_regularizer=l2(1e-4))(input)\n","        else:\n","            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n","                                  strides=init_strides)(input)\n","\n","        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n","        return _shortcut(input, residual)\n","\n","    return f\n","\n","\n","def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n","    \"\"\"Bottleneck architecture for > 34 layer resnet.\n","    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n","    Returns:\n","        A final conv layer of filters * 4\n","    \"\"\"\n","    def f(input):\n","\n","        if is_first_block_of_first_layer:\n","            # don't repeat bn->relu since we just did bn->relu->maxpool\n","            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n","                              strides=init_strides,\n","                              padding=\"same\",\n","                              kernel_initializer=\"he_normal\",\n","                              kernel_regularizer=l2(1e-4))(input)\n","        else:\n","            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n","                                     strides=init_strides)(input)\n","\n","        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n","        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n","        return _shortcut(input, residual)\n","\n","    return f\n","\n","\n","def _handle_dim_ordering():\n","    global ROW_AXIS\n","    global COL_AXIS\n","    global CHANNEL_AXIS\n","    if K.image_dim_ordering() == 'tf':\n","        ROW_AXIS = 1\n","        COL_AXIS = 2\n","        CHANNEL_AXIS = 3\n","    else:\n","        CHANNEL_AXIS = 1\n","        ROW_AXIS = 2\n","        COL_AXIS = 3\n","\n","\n","def _get_block(identifier):\n","    if isinstance(identifier, six.string_types):\n","        res = globals().get(identifier)\n","        if not res:\n","            raise ValueError('Invalid {}'.format(identifier))\n","        return res\n","    return identifier\n","\n","\n","class ResnetBuilder(object):\n","    @staticmethod\n","    def build(input_shape, num_outputs, block_fn, repetitions):\n","        \"\"\"Builds a custom ResNet like architecture.\n","        Args:\n","            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n","            num_outputs: The number of outputs at final softmax layer\n","            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n","                The original paper used basic_block for layers < 50\n","            repetitions: Number of repetitions of various block units.\n","                At each block unit, the number of filters are doubled and the input size is halved\n","        Returns:\n","            The keras `Model`.\n","        \"\"\"\n","        _handle_dim_ordering()\n","        if len(input_shape) != 3:\n","            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n","\n","        # Permute dimension order if necessary\n","        if K.image_dim_ordering() == 'tf':\n","            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n","\n","        # Load function from str if needed.\n","        block_fn = _get_block(block_fn)\n","\n","        input = Input(shape=input_shape)\n","        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n","        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n","\n","        block = pool1\n","        filters = 64\n","        for i, r in enumerate(repetitions):\n","            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n","            filters *= 2\n","\n","        # Last activation\n","        block = _bn_relu(block)\n","        \n","        \n","        final_layer = Conv2D(num_outputs, (1,1), strides=(1,1), name='conv_1x1', use_bias=False)(block)\n","        \n","#         # Classifier block\n","        block_shape = K.int_shape(block)\n","        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]), strides=(1, 1))(final_layer)\n","        flatten1 = Flatten()(pool2)\n","        output = Activation('softmax')(flatten1)\n","        \n","#         final_layer = Conv2D(num_outputs, (1,1), strides=(1,1), name='conv_1x1', use_bias=False)(flatten1)\n","# #         dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",activation=\"softmax\")(flatten1)\n","\n","        model = Model(inputs=input, outputs=output)\n","        return model\n","\n","    @staticmethod\n","    def build_resnet_50(input_shape, num_outputs):\n","        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n","      \n","model = ResnetBuilder.build_resnet_50((3,64,64),200)\n","model.summary()"],"execution_count":34,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            (None, 64, 64, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_266 (Conv2D)             (None, 32, 32, 64)   9472        input_6[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_246 (BatchN (None, 32, 32, 64)   256         conv2d_266[0][0]                 \n","__________________________________________________________________________________________________\n","activation_250 (Activation)     (None, 32, 32, 64)   0           batch_normalization_246[0][0]    \n","__________________________________________________________________________________________________\n","max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_250[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_267 (Conv2D)             (None, 16, 16, 64)   4160        max_pooling2d_6[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_247 (BatchN (None, 16, 16, 64)   256         conv2d_267[0][0]                 \n","__________________________________________________________________________________________________\n","activation_251 (Activation)     (None, 16, 16, 64)   0           batch_normalization_247[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_268 (Conv2D)             (None, 16, 16, 64)   36928       activation_251[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_248 (BatchN (None, 16, 16, 64)   256         conv2d_268[0][0]                 \n","__________________________________________________________________________________________________\n","activation_252 (Activation)     (None, 16, 16, 64)   0           batch_normalization_248[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_270 (Conv2D)             (None, 16, 16, 256)  16640       max_pooling2d_6[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_269 (Conv2D)             (None, 16, 16, 256)  16640       activation_252[0][0]             \n","__________________________________________________________________________________________________\n","add_81 (Add)                    (None, 16, 16, 256)  0           conv2d_270[0][0]                 \n","                                                                 conv2d_269[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_249 (BatchN (None, 16, 16, 256)  1024        add_81[0][0]                     \n","__________________________________________________________________________________________________\n","activation_253 (Activation)     (None, 16, 16, 256)  0           batch_normalization_249[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_271 (Conv2D)             (None, 16, 16, 64)   16448       activation_253[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_250 (BatchN (None, 16, 16, 64)   256         conv2d_271[0][0]                 \n","__________________________________________________________________________________________________\n","activation_254 (Activation)     (None, 16, 16, 64)   0           batch_normalization_250[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_272 (Conv2D)             (None, 16, 16, 64)   36928       activation_254[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_251 (BatchN (None, 16, 16, 64)   256         conv2d_272[0][0]                 \n","__________________________________________________________________________________________________\n","activation_255 (Activation)     (None, 16, 16, 64)   0           batch_normalization_251[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_273 (Conv2D)             (None, 16, 16, 256)  16640       activation_255[0][0]             \n","__________________________________________________________________________________________________\n","add_82 (Add)                    (None, 16, 16, 256)  0           add_81[0][0]                     \n","                                                                 conv2d_273[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_252 (BatchN (None, 16, 16, 256)  1024        add_82[0][0]                     \n","__________________________________________________________________________________________________\n","activation_256 (Activation)     (None, 16, 16, 256)  0           batch_normalization_252[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_274 (Conv2D)             (None, 16, 16, 64)   16448       activation_256[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_253 (BatchN (None, 16, 16, 64)   256         conv2d_274[0][0]                 \n","__________________________________________________________________________________________________\n","activation_257 (Activation)     (None, 16, 16, 64)   0           batch_normalization_253[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_275 (Conv2D)             (None, 16, 16, 64)   36928       activation_257[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_254 (BatchN (None, 16, 16, 64)   256         conv2d_275[0][0]                 \n","__________________________________________________________________________________________________\n","activation_258 (Activation)     (None, 16, 16, 64)   0           batch_normalization_254[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_276 (Conv2D)             (None, 16, 16, 256)  16640       activation_258[0][0]             \n","__________________________________________________________________________________________________\n","add_83 (Add)                    (None, 16, 16, 256)  0           add_82[0][0]                     \n","                                                                 conv2d_276[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_255 (BatchN (None, 16, 16, 256)  1024        add_83[0][0]                     \n","__________________________________________________________________________________________________\n","activation_259 (Activation)     (None, 16, 16, 256)  0           batch_normalization_255[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_277 (Conv2D)             (None, 8, 8, 128)    32896       activation_259[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_256 (BatchN (None, 8, 8, 128)    512         conv2d_277[0][0]                 \n","__________________________________________________________________________________________________\n","activation_260 (Activation)     (None, 8, 8, 128)    0           batch_normalization_256[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_278 (Conv2D)             (None, 8, 8, 128)    147584      activation_260[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_257 (BatchN (None, 8, 8, 128)    512         conv2d_278[0][0]                 \n","__________________________________________________________________________________________________\n","activation_261 (Activation)     (None, 8, 8, 128)    0           batch_normalization_257[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_280 (Conv2D)             (None, 8, 8, 512)    131584      add_83[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_279 (Conv2D)             (None, 8, 8, 512)    66048       activation_261[0][0]             \n","__________________________________________________________________________________________________\n","add_84 (Add)                    (None, 8, 8, 512)    0           conv2d_280[0][0]                 \n","                                                                 conv2d_279[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_258 (BatchN (None, 8, 8, 512)    2048        add_84[0][0]                     \n","__________________________________________________________________________________________________\n","activation_262 (Activation)     (None, 8, 8, 512)    0           batch_normalization_258[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_281 (Conv2D)             (None, 8, 8, 128)    65664       activation_262[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_259 (BatchN (None, 8, 8, 128)    512         conv2d_281[0][0]                 \n","__________________________________________________________________________________________________\n","activation_263 (Activation)     (None, 8, 8, 128)    0           batch_normalization_259[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_282 (Conv2D)             (None, 8, 8, 128)    147584      activation_263[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_260 (BatchN (None, 8, 8, 128)    512         conv2d_282[0][0]                 \n","__________________________________________________________________________________________________\n","activation_264 (Activation)     (None, 8, 8, 128)    0           batch_normalization_260[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_283 (Conv2D)             (None, 8, 8, 512)    66048       activation_264[0][0]             \n","__________________________________________________________________________________________________\n","add_85 (Add)                    (None, 8, 8, 512)    0           add_84[0][0]                     \n","                                                                 conv2d_283[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_261 (BatchN (None, 8, 8, 512)    2048        add_85[0][0]                     \n","__________________________________________________________________________________________________\n","activation_265 (Activation)     (None, 8, 8, 512)    0           batch_normalization_261[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_284 (Conv2D)             (None, 8, 8, 128)    65664       activation_265[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_262 (BatchN (None, 8, 8, 128)    512         conv2d_284[0][0]                 \n","__________________________________________________________________________________________________\n","activation_266 (Activation)     (None, 8, 8, 128)    0           batch_normalization_262[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_285 (Conv2D)             (None, 8, 8, 128)    147584      activation_266[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_263 (BatchN (None, 8, 8, 128)    512         conv2d_285[0][0]                 \n","__________________________________________________________________________________________________\n","activation_267 (Activation)     (None, 8, 8, 128)    0           batch_normalization_263[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_286 (Conv2D)             (None, 8, 8, 512)    66048       activation_267[0][0]             \n","__________________________________________________________________________________________________\n","add_86 (Add)                    (None, 8, 8, 512)    0           add_85[0][0]                     \n","                                                                 conv2d_286[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_264 (BatchN (None, 8, 8, 512)    2048        add_86[0][0]                     \n","__________________________________________________________________________________________________\n","activation_268 (Activation)     (None, 8, 8, 512)    0           batch_normalization_264[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_287 (Conv2D)             (None, 8, 8, 128)    65664       activation_268[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_265 (BatchN (None, 8, 8, 128)    512         conv2d_287[0][0]                 \n","__________________________________________________________________________________________________\n","activation_269 (Activation)     (None, 8, 8, 128)    0           batch_normalization_265[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_288 (Conv2D)             (None, 8, 8, 128)    147584      activation_269[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_266 (BatchN (None, 8, 8, 128)    512         conv2d_288[0][0]                 \n","__________________________________________________________________________________________________\n","activation_270 (Activation)     (None, 8, 8, 128)    0           batch_normalization_266[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_289 (Conv2D)             (None, 8, 8, 512)    66048       activation_270[0][0]             \n","__________________________________________________________________________________________________\n","add_87 (Add)                    (None, 8, 8, 512)    0           add_86[0][0]                     \n","                                                                 conv2d_289[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_267 (BatchN (None, 8, 8, 512)    2048        add_87[0][0]                     \n","__________________________________________________________________________________________________\n","activation_271 (Activation)     (None, 8, 8, 512)    0           batch_normalization_267[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_290 (Conv2D)             (None, 4, 4, 256)    131328      activation_271[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_268 (BatchN (None, 4, 4, 256)    1024        conv2d_290[0][0]                 \n","__________________________________________________________________________________________________\n","activation_272 (Activation)     (None, 4, 4, 256)    0           batch_normalization_268[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_291 (Conv2D)             (None, 4, 4, 256)    590080      activation_272[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_269 (BatchN (None, 4, 4, 256)    1024        conv2d_291[0][0]                 \n","__________________________________________________________________________________________________\n","activation_273 (Activation)     (None, 4, 4, 256)    0           batch_normalization_269[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_293 (Conv2D)             (None, 4, 4, 1024)   525312      add_87[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_292 (Conv2D)             (None, 4, 4, 1024)   263168      activation_273[0][0]             \n","__________________________________________________________________________________________________\n","add_88 (Add)                    (None, 4, 4, 1024)   0           conv2d_293[0][0]                 \n","                                                                 conv2d_292[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_270 (BatchN (None, 4, 4, 1024)   4096        add_88[0][0]                     \n","__________________________________________________________________________________________________\n","activation_274 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_270[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_294 (Conv2D)             (None, 4, 4, 256)    262400      activation_274[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_271 (BatchN (None, 4, 4, 256)    1024        conv2d_294[0][0]                 \n","__________________________________________________________________________________________________\n","activation_275 (Activation)     (None, 4, 4, 256)    0           batch_normalization_271[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_295 (Conv2D)             (None, 4, 4, 256)    590080      activation_275[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_272 (BatchN (None, 4, 4, 256)    1024        conv2d_295[0][0]                 \n","__________________________________________________________________________________________________\n","activation_276 (Activation)     (None, 4, 4, 256)    0           batch_normalization_272[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_296 (Conv2D)             (None, 4, 4, 1024)   263168      activation_276[0][0]             \n","__________________________________________________________________________________________________\n","add_89 (Add)                    (None, 4, 4, 1024)   0           add_88[0][0]                     \n","                                                                 conv2d_296[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_273 (BatchN (None, 4, 4, 1024)   4096        add_89[0][0]                     \n","__________________________________________________________________________________________________\n","activation_277 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_273[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_297 (Conv2D)             (None, 4, 4, 256)    262400      activation_277[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_274 (BatchN (None, 4, 4, 256)    1024        conv2d_297[0][0]                 \n","__________________________________________________________________________________________________\n","activation_278 (Activation)     (None, 4, 4, 256)    0           batch_normalization_274[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_298 (Conv2D)             (None, 4, 4, 256)    590080      activation_278[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_275 (BatchN (None, 4, 4, 256)    1024        conv2d_298[0][0]                 \n","__________________________________________________________________________________________________\n","activation_279 (Activation)     (None, 4, 4, 256)    0           batch_normalization_275[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_299 (Conv2D)             (None, 4, 4, 1024)   263168      activation_279[0][0]             \n","__________________________________________________________________________________________________\n","add_90 (Add)                    (None, 4, 4, 1024)   0           add_89[0][0]                     \n","                                                                 conv2d_299[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_276 (BatchN (None, 4, 4, 1024)   4096        add_90[0][0]                     \n","__________________________________________________________________________________________________\n","activation_280 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_276[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_300 (Conv2D)             (None, 4, 4, 256)    262400      activation_280[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_277 (BatchN (None, 4, 4, 256)    1024        conv2d_300[0][0]                 \n","__________________________________________________________________________________________________\n","activation_281 (Activation)     (None, 4, 4, 256)    0           batch_normalization_277[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_301 (Conv2D)             (None, 4, 4, 256)    590080      activation_281[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_278 (BatchN (None, 4, 4, 256)    1024        conv2d_301[0][0]                 \n","__________________________________________________________________________________________________\n","activation_282 (Activation)     (None, 4, 4, 256)    0           batch_normalization_278[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_302 (Conv2D)             (None, 4, 4, 1024)   263168      activation_282[0][0]             \n","__________________________________________________________________________________________________\n","add_91 (Add)                    (None, 4, 4, 1024)   0           add_90[0][0]                     \n","                                                                 conv2d_302[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_279 (BatchN (None, 4, 4, 1024)   4096        add_91[0][0]                     \n","__________________________________________________________________________________________________\n","activation_283 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_279[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_303 (Conv2D)             (None, 4, 4, 256)    262400      activation_283[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_280 (BatchN (None, 4, 4, 256)    1024        conv2d_303[0][0]                 \n","__________________________________________________________________________________________________\n","activation_284 (Activation)     (None, 4, 4, 256)    0           batch_normalization_280[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_304 (Conv2D)             (None, 4, 4, 256)    590080      activation_284[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_281 (BatchN (None, 4, 4, 256)    1024        conv2d_304[0][0]                 \n","__________________________________________________________________________________________________\n","activation_285 (Activation)     (None, 4, 4, 256)    0           batch_normalization_281[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_305 (Conv2D)             (None, 4, 4, 1024)   263168      activation_285[0][0]             \n","__________________________________________________________________________________________________\n","add_92 (Add)                    (None, 4, 4, 1024)   0           add_91[0][0]                     \n","                                                                 conv2d_305[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_282 (BatchN (None, 4, 4, 1024)   4096        add_92[0][0]                     \n","__________________________________________________________________________________________________\n","activation_286 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_282[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_306 (Conv2D)             (None, 4, 4, 256)    262400      activation_286[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_283 (BatchN (None, 4, 4, 256)    1024        conv2d_306[0][0]                 \n","__________________________________________________________________________________________________\n","activation_287 (Activation)     (None, 4, 4, 256)    0           batch_normalization_283[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_307 (Conv2D)             (None, 4, 4, 256)    590080      activation_287[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_284 (BatchN (None, 4, 4, 256)    1024        conv2d_307[0][0]                 \n","__________________________________________________________________________________________________\n","activation_288 (Activation)     (None, 4, 4, 256)    0           batch_normalization_284[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_308 (Conv2D)             (None, 4, 4, 1024)   263168      activation_288[0][0]             \n","__________________________________________________________________________________________________\n","add_93 (Add)                    (None, 4, 4, 1024)   0           add_92[0][0]                     \n","                                                                 conv2d_308[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_285 (BatchN (None, 4, 4, 1024)   4096        add_93[0][0]                     \n","__________________________________________________________________________________________________\n","activation_289 (Activation)     (None, 4, 4, 1024)   0           batch_normalization_285[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_309 (Conv2D)             (None, 2, 2, 512)    524800      activation_289[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_286 (BatchN (None, 2, 2, 512)    2048        conv2d_309[0][0]                 \n","__________________________________________________________________________________________________\n","activation_290 (Activation)     (None, 2, 2, 512)    0           batch_normalization_286[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_310 (Conv2D)             (None, 2, 2, 512)    2359808     activation_290[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_287 (BatchN (None, 2, 2, 512)    2048        conv2d_310[0][0]                 \n","__________________________________________________________________________________________________\n","activation_291 (Activation)     (None, 2, 2, 512)    0           batch_normalization_287[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_312 (Conv2D)             (None, 2, 2, 2048)   2099200     add_93[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_311 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_291[0][0]             \n","__________________________________________________________________________________________________\n","add_94 (Add)                    (None, 2, 2, 2048)   0           conv2d_312[0][0]                 \n","                                                                 conv2d_311[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_288 (BatchN (None, 2, 2, 2048)   8192        add_94[0][0]                     \n","__________________________________________________________________________________________________\n","activation_292 (Activation)     (None, 2, 2, 2048)   0           batch_normalization_288[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_313 (Conv2D)             (None, 2, 2, 512)    1049088     activation_292[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_289 (BatchN (None, 2, 2, 512)    2048        conv2d_313[0][0]                 \n","__________________________________________________________________________________________________\n","activation_293 (Activation)     (None, 2, 2, 512)    0           batch_normalization_289[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_314 (Conv2D)             (None, 2, 2, 512)    2359808     activation_293[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_290 (BatchN (None, 2, 2, 512)    2048        conv2d_314[0][0]                 \n","__________________________________________________________________________________________________\n","activation_294 (Activation)     (None, 2, 2, 512)    0           batch_normalization_290[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_315 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_294[0][0]             \n","__________________________________________________________________________________________________\n","add_95 (Add)                    (None, 2, 2, 2048)   0           add_94[0][0]                     \n","                                                                 conv2d_315[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_291 (BatchN (None, 2, 2, 2048)   8192        add_95[0][0]                     \n","__________________________________________________________________________________________________\n","activation_295 (Activation)     (None, 2, 2, 2048)   0           batch_normalization_291[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_316 (Conv2D)             (None, 2, 2, 512)    1049088     activation_295[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_292 (BatchN (None, 2, 2, 512)    2048        conv2d_316[0][0]                 \n","__________________________________________________________________________________________________\n","activation_296 (Activation)     (None, 2, 2, 512)    0           batch_normalization_292[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_317 (Conv2D)             (None, 2, 2, 512)    2359808     activation_296[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_293 (BatchN (None, 2, 2, 512)    2048        conv2d_317[0][0]                 \n","__________________________________________________________________________________________________\n","activation_297 (Activation)     (None, 2, 2, 512)    0           batch_normalization_293[0][0]    \n","__________________________________________________________________________________________________\n","conv2d_318 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_297[0][0]             \n","__________________________________________________________________________________________________\n","add_96 (Add)                    (None, 2, 2, 2048)   0           add_95[0][0]                     \n","                                                                 conv2d_318[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_294 (BatchN (None, 2, 2, 2048)   8192        add_96[0][0]                     \n","__________________________________________________________________________________________________\n","activation_298 (Activation)     (None, 2, 2, 2048)   0           batch_normalization_294[0][0]    \n","__________________________________________________________________________________________________\n","conv_1x1 (Conv2D)               (None, 2, 2, 200)    409600      activation_298[0][0]             \n","__________________________________________________________________________________________________\n","average_pooling2d_2 (AveragePoo (None, 1, 1, 200)    0           conv_1x1[0][0]                   \n","__________________________________________________________________________________________________\n","flatten_5 (Flatten)             (None, 200)          0           average_pooling2d_2[0][0]        \n","__________________________________________________________________________________________________\n","activation_299 (Activation)     (None, 200)          0           flatten_5[0][0]                  \n","==================================================================================================\n","Total params: 23,981,952\n","Trainable params: 23,936,512\n","Non-trainable params: 45,440\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"WP2SmSAIACOj","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","from keras.callbacks import *\n","\n","class CyclicLR(Callback):\n","    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n","    The method cycles the learning rate between two boundaries with\n","    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n","    The amplitude of the cycle can be scaled on a per-iteration or \n","    per-cycle basis.\n","    This class has three built-in policies, as put forth in the paper.\n","    \"triangular\":\n","        A basic triangular cycle w/ no amplitude scaling.\n","    \"triangular2\":\n","        A basic triangular cycle that scales initial amplitude by half each cycle.\n","    \"exp_range\":\n","        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n","        cycle iteration.\n","    For more detail, please see paper.\n","    \n","    # Example\n","        ```python\n","            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","                                step_size=2000., mode='triangular')\n","            model.fit(X_train, Y_train, callbacks=[clr])\n","        ```\n","    \n","    Class also supports custom scaling functions:\n","        ```python\n","            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n","            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","                                step_size=2000., scale_fn=clr_fn,\n","                                scale_mode='cycle')\n","            model.fit(X_train, Y_train, callbacks=[clr])\n","        ```    \n","    # Arguments\n","        base_lr: initial learning rate which is the\n","            lower boundary in the cycle.\n","        max_lr: upper boundary in the cycle. Functionally,\n","            it defines the cycle amplitude (max_lr - base_lr).\n","            The lr at any cycle is the sum of base_lr\n","            and some scaling of the amplitude; therefore \n","            max_lr may not actually be reached depending on\n","            scaling function.\n","        step_size: number of training iterations per\n","            half cycle. Authors suggest setting step_size\n","            2-8 x training iterations in epoch.\n","        mode: one of {triangular, triangular2, exp_range}.\n","            Default 'triangular'.\n","            Values correspond to policies detailed above.\n","            If scale_fn is not None, this argument is ignored.\n","        gamma: constant in 'exp_range' scaling function:\n","            gamma**(cycle iterations)\n","        scale_fn: Custom scaling policy defined by a single\n","            argument lambda function, where \n","            0 <= scale_fn(x) <= 1 for all x >= 0.\n","            mode paramater is ignored \n","        scale_mode: {'cycle', 'iterations'}.\n","            Defines whether scale_fn is evaluated on \n","            cycle number or cycle iterations (training\n","            iterations since start of cycle). Default is 'cycle'.\n","    \"\"\"\n","\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1/(2.**(x-1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma**(x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","        \n","    def clr(self):\n","        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n","        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n","        \n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())        \n","            \n","    def on_batch_end(self, epoch, logs=None):\n","        \n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        self.history.setdefault('iterations', []).append(self.trn_iterations)\n","\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","        \n","        K.set_value(self.model.optimizer.lr, self.clr())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DOUuuC5xwtIs","colab_type":"code","outputId":"4050ca22-55da-4c3f-b94f-8918425b3758","executionInfo":{"status":"ok","timestamp":1554035659631,"user_tz":-330,"elapsed":24774,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"myl0Fkg8xw7F","colab_type":"code","outputId":"dab830c5-f3dc-4b4f-dbe0-95e12e3314bd","executionInfo":{"status":"ok","timestamp":1553862823387,"user_tz":-330,"elapsed":107123,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["import os\n","\n","dir_path = os.listdir('/content/gdrive/My Drive/EIP3.0')\n","print(dir_path)\n","\n","model.save_weights(\"/content/gdrive/My Drive/EIP3.0/model_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n","print(\"Saved model to disk\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['Assignment 2A2B.ipynb', 'Assignment 3B.ipynb', '1st DNN.ipynb', 'Assignment 3C.ipynb', 'weights-improvement-13-0.15.hdf5', 'model.json']\n","Saved model to disk\n"],"name":"stdout"}]},{"metadata":{"id":"vxIBpxX7D-OK","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import optimizers\n","\n","clr_triangular = CyclicLR(mode='triangular')\n","\n","epochs = 50\n","learning_rate = 0.01\n","decay_rate = learning_rate / epochs\n","momentum = 0.9\n","# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n","sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy',\n","             optimizer=sgd,\n","             metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_WIbCa8WqCu","colab_type":"code","outputId":"4f2cfeff-63c3-4028-914b-4dd48efe31d9","executionInfo":{"status":"ok","timestamp":1554049440356,"user_tz":-330,"elapsed":13686360,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":1958}},"cell_type":"code","source":["for epoch in range(epochs):  \n","  model.fit_generator(train_generator,epochs=1, callbacks=[clr_triangular], samples_per_epoch = 100000, verbose=1)\n","  if epoch % 10 == 0:\n","    results = model.evaluate_generator(validation_generator, steps=1) \n","    print(\"epoch \"+str(epoch)+\" of \"+str(epochs))\n","    print('Accuracy :', (results[1]*100.0))\n","\n","    \n","model.save_weights(\"/content/gdrive/My Drive/EIP3.0/model_64x64_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n","print(\"Saved model to disk\")\n","\n","# model.save_weights(\"./EIP3.0/model_\"+epcoch+\"_\"+results+\".h5\")\n","# print(\"Saved model to disk\")"],"execution_count":37,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=1, callbacks=[<__main__..., verbose=1, steps_per_epoch=200)`\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","200/200 [==============================] - 276s 1s/step - loss: 10.5998 - acc: 0.0135\n","epoch 0 of 50\n","Accuracy : 2.500000037252903\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 10.3468 - acc: 0.0321\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 10.1818 - acc: 0.0459\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 10.0568 - acc: 0.0567\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 9.9555 - acc: 0.0652\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 9.8584 - acc: 0.0753\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 9.7608 - acc: 0.0844\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 9.6685 - acc: 0.0946\n","Epoch 1/1\n","200/200 [==============================] - 268s 1s/step - loss: 9.5617 - acc: 0.1042\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 9.4577 - acc: 0.1132\n","Epoch 1/1\n","200/200 [==============================] - 272s 1s/step - loss: 9.3554 - acc: 0.1209\n","epoch 10 of 50\n","Accuracy : 8.500000089406967\n","Epoch 1/1\n","200/200 [==============================] - 273s 1s/step - loss: 9.2714 - acc: 0.1303\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 9.1873 - acc: 0.1393\n","Epoch 1/1\n","200/200 [==============================] - 273s 1s/step - loss: 9.1261 - acc: 0.1457\n","Epoch 1/1\n","200/200 [==============================] - 273s 1s/step - loss: 9.0666 - acc: 0.1509\n","Epoch 1/1\n","200/200 [==============================] - 273s 1s/step - loss: 9.0191 - acc: 0.1563\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.9694 - acc: 0.1596\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.9339 - acc: 0.1661\n","Epoch 1/1\n","200/200 [==============================] - 272s 1s/step - loss: 8.9099 - acc: 0.1666\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.8864 - acc: 0.1705\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.8683 - acc: 0.1734\n","epoch 20 of 50\n","Accuracy : 11.999999731779099\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.8591 - acc: 0.1733\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 8.8385 - acc: 0.1738\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.8135 - acc: 0.1760\n","Epoch 1/1\n","200/200 [==============================] - 272s 1s/step - loss: 8.7831 - acc: 0.1776\n","Epoch 1/1\n","200/200 [==============================] - 272s 1s/step - loss: 8.7589 - acc: 0.1795\n","Epoch 1/1\n","200/200 [==============================] - 273s 1s/step - loss: 8.7158 - acc: 0.1839\n","Epoch 1/1\n","200/200 [==============================] - 274s 1s/step - loss: 8.6710 - acc: 0.1882\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.6273 - acc: 0.1924\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.5762 - acc: 0.1956\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.5249 - acc: 0.1985\n","epoch 30 of 50\n","Accuracy : 13.500000536441803\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.4651 - acc: 0.2061\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.4099 - acc: 0.2128\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.3653 - acc: 0.2174\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.3262 - acc: 0.2192\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.2835 - acc: 0.2259\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.2515 - acc: 0.2286\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.2221 - acc: 0.2317\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.1941 - acc: 0.2366\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 8.1722 - acc: 0.2382\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.1617 - acc: 0.2389\n","epoch 40 of 50\n","Accuracy : 19.499999284744263\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.1519 - acc: 0.2408\n","Epoch 1/1\n","200/200 [==============================] - 269s 1s/step - loss: 8.1467 - acc: 0.2393\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.1215 - acc: 0.2399\n","Epoch 1/1\n","200/200 [==============================] - 272s 1s/step - loss: 8.1101 - acc: 0.2434\n","Epoch 1/1\n","200/200 [==============================] - 271s 1s/step - loss: 8.0829 - acc: 0.2440\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.0565 - acc: 0.2453\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 8.0241 - acc: 0.2476\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 7.9894 - acc: 0.2515\n","Epoch 1/1\n","200/200 [==============================] - 270s 1s/step - loss: 7.9594 - acc: 0.2519\n","Saved model to disk\n"],"name":"stdout"}]},{"metadata":{"id":"6GqNcP5iLZn2","colab_type":"code","colab":{}},"cell_type":"code","source":["#load weights to model\n","model.load_weights(\"/content/gdrive/My Drive/EIP3.0/model_49_0.18000000715255737.h5\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gRQMWHiwkBEi","colab_type":"code","outputId":"6cf43abb-5481-4a1d-80d2-22ee13f898a9","executionInfo":{"status":"ok","timestamp":1553964003475,"user_tz":-330,"elapsed":14656766,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":5341}},"cell_type":"code","source":["from keras import optimizers\n","\n","clr_triangular = CyclicLR(mode='triangular')\n","\n","epochs = 50\n","learning_rate = 0.01\n","decay_rate = learning_rate / epochs\n","momentum = 0.9\n","# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n","sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy',\n","             optimizer=sgd,\n","             metrics=['accuracy'])\n","epochs = 100\n","for epoch in range(epochs):  \n","  print(\"50 to 150 epochs\")\n","  model.fit_generator(train_generator,epochs=1, callbacks=[clr_triangular], samples_per_epoch = 100000, verbose=1)\n","  if epoch % 10 == 0:\n","    results = model.evaluate_generator(validation_generator, steps=1) \n","    print('Accuracy :', (results[1]*100.0))\n","\n","\n","model.save_weights(\"/content/gdrive/My Drive/EIP3.0/model_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n","print(\"Saved model to disk\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["50 to 150 epochs\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=1, callbacks=[<__main__..., verbose=1, steps_per_epoch=200)`\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/1\n","200/200 [==============================] - 157s 786ms/step - loss: 7.9973 - acc: 0.2429\n","Accuracy : 17.499999701976776\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.9788 - acc: 0.2473\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 767ms/step - loss: 7.9631 - acc: 0.2476\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 766ms/step - loss: 7.9449 - acc: 0.2495\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.9487 - acc: 0.2472\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 763ms/step - loss: 7.9146 - acc: 0.2499\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 762ms/step - loss: 7.8949 - acc: 0.2493\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 762ms/step - loss: 7.8694 - acc: 0.2518\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 766ms/step - loss: 7.8463 - acc: 0.2526\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 154s 770ms/step - loss: 7.8173 - acc: 0.2525\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 154s 769ms/step - loss: 7.7769 - acc: 0.2556\n","Accuracy : 17.000000178813934\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 763ms/step - loss: 7.7272 - acc: 0.2616\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 154s 768ms/step - loss: 7.6838 - acc: 0.2648\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 154s 772ms/step - loss: 7.6425 - acc: 0.2719\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 760ms/step - loss: 7.5954 - acc: 0.2756\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 764ms/step - loss: 7.5649 - acc: 0.2800\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 766ms/step - loss: 7.5269 - acc: 0.2848\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 761ms/step - loss: 7.5000 - acc: 0.2872\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 764ms/step - loss: 7.4710 - acc: 0.2937\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.4427 - acc: 0.2978\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 759ms/step - loss: 7.4288 - acc: 0.2990\n","Accuracy : 17.000000178813934\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.4214 - acc: 0.3008\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 762ms/step - loss: 7.4162 - acc: 0.2975\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.4054 - acc: 0.2969\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 762ms/step - loss: 7.3918 - acc: 0.2993\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 761ms/step - loss: 7.3817 - acc: 0.2991\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 766ms/step - loss: 7.3528 - acc: 0.3013\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.3418 - acc: 0.2966\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 154s 771ms/step - loss: 7.3152 - acc: 0.3006\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 767ms/step - loss: 7.2977 - acc: 0.2988\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 150s 748ms/step - loss: 7.2660 - acc: 0.3005\n","Accuracy : 14.000000059604645\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 151s 755ms/step - loss: 7.2219 - acc: 0.3063\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 155s 774ms/step - loss: 7.1727 - acc: 0.3141\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 152s 760ms/step - loss: 7.1334 - acc: 0.3180\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 153s 765ms/step - loss: 7.0947 - acc: 0.3227\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 148s 741ms/step - loss: 7.0558 - acc: 0.3295\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 147s 737ms/step - loss: 7.0195 - acc: 0.3322\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 145s 725ms/step - loss: 6.9800 - acc: 0.3403\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 145s 726ms/step - loss: 6.9596 - acc: 0.3421\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 144s 721ms/step - loss: 6.9205 - acc: 0.3498\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 146s 729ms/step - loss: 6.8914 - acc: 0.3538\n","Accuracy : 20.999999344348907\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 148s 739ms/step - loss: 6.9034 - acc: 0.3511\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 143s 713ms/step - loss: 6.8874 - acc: 0.3528\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 702ms/step - loss: 6.8822 - acc: 0.3518\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 711ms/step - loss: 6.8741 - acc: 0.3495\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 698ms/step - loss: 6.8613 - acc: 0.3507\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 703ms/step - loss: 6.8479 - acc: 0.3499\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 698ms/step - loss: 6.8331 - acc: 0.3494\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 702ms/step - loss: 6.8232 - acc: 0.3469\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 149s 743ms/step - loss: 6.8681 - acc: 0.3316\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 148s 741ms/step - loss: 6.8185 - acc: 0.3420\n","Accuracy : 17.000000178813934\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 145s 726ms/step - loss: 6.7790 - acc: 0.3454\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 710ms/step - loss: 6.7223 - acc: 0.3538\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 711ms/step - loss: 6.6644 - acc: 0.3625\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 696ms/step - loss: 6.6158 - acc: 0.3696\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 709ms/step - loss: 6.5690 - acc: 0.3774\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 700ms/step - loss: 6.5323 - acc: 0.3826\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 144s 718ms/step - loss: 6.4829 - acc: 0.3948\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 148s 739ms/step - loss: 6.4401 - acc: 0.3994\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 150s 752ms/step - loss: 6.4098 - acc: 0.4056\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 146s 732ms/step - loss: 6.3922 - acc: 0.4093\n","Accuracy : 15.50000011920929\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 147s 735ms/step - loss: 6.3797 - acc: 0.4113\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 144s 722ms/step - loss: 6.3885 - acc: 0.4070\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 711ms/step - loss: 6.4016 - acc: 0.4030\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 708ms/step - loss: 6.3904 - acc: 0.4001\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 708ms/step - loss: 6.4070 - acc: 0.3950\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 143s 714ms/step - loss: 6.3930 - acc: 0.3931\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 712ms/step - loss: 6.3836 - acc: 0.3926\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 143s 714ms/step - loss: 6.3764 - acc: 0.3895\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 699ms/step - loss: 6.3728 - acc: 0.3875\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 704ms/step - loss: 6.3393 - acc: 0.3885\n","Accuracy : 17.499999701976776\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 694ms/step - loss: 6.2777 - acc: 0.3975\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 707ms/step - loss: 6.2290 - acc: 0.4066\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 138s 691ms/step - loss: 6.1714 - acc: 0.4142\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 138s 688ms/step - loss: 6.1144 - acc: 0.4273\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 138s 688ms/step - loss: 6.0562 - acc: 0.4379\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 698ms/step - loss: 6.0014 - acc: 0.4484\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 697ms/step - loss: 5.9515 - acc: 0.4585\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 138s 691ms/step - loss: 5.9056 - acc: 0.4695\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 694ms/step - loss: 5.8667 - acc: 0.4783\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 699ms/step - loss: 5.8373 - acc: 0.4850\n","Accuracy : 21.99999988079071\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 143s 714ms/step - loss: 5.8387 - acc: 0.4821\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 693ms/step - loss: 5.8526 - acc: 0.4776\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 704ms/step - loss: 5.8547 - acc: 0.4726\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 709ms/step - loss: 5.8614 - acc: 0.4676\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 707ms/step - loss: 5.8663 - acc: 0.4649\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 144s 721ms/step - loss: 5.8733 - acc: 0.4575\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 145s 727ms/step - loss: 5.8806 - acc: 0.4509\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 144s 721ms/step - loss: 5.8796 - acc: 0.4471\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 708ms/step - loss: 5.8802 - acc: 0.4442\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 709ms/step - loss: 5.8598 - acc: 0.4454\n","Accuracy : 12.999999523162842\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 712ms/step - loss: 5.7841 - acc: 0.4599\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 142s 711ms/step - loss: 5.7299 - acc: 0.4678\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 693ms/step - loss: 5.6659 - acc: 0.4821\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 140s 698ms/step - loss: 5.5912 - acc: 0.4975\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 146s 728ms/step - loss: 5.5296 - acc: 0.5108\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 704ms/step - loss: 5.4785 - acc: 0.5214\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 139s 696ms/step - loss: 5.4113 - acc: 0.5357\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 138s 691ms/step - loss: 5.3478 - acc: 0.5511\n","50 to 150 epochs\n","Epoch 1/1\n","200/200 [==============================] - 141s 706ms/step - loss: 5.3059 - acc: 0.5625\n","Saved model to disk\n"],"name":"stdout"}]},{"metadata":{"id":"62OXkotvsRst","colab_type":"code","outputId":"f717cc63-6b7a-44ce-c43d-7c0208172491","executionInfo":{"status":"error","timestamp":1553972227933,"user_tz":-330,"elapsed":6714,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":860}},"cell_type":"code","source":["#initialize train and val data generator\n","train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64, 64), color_mode='rgb', \n","                                                    batch_size=500, class_mode='categorical', shuffle=True, seed=42)\n","validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64, 64),\n","                                                    color_mode='rgb', class_mode='categorical', batch_size=200, shuffle=True, seed=42)\n","\n","model = ResnetBuilder.build_resnet_50((3,64,64),200)\n","#load weights to model\n","model.load_weights(\"/content/gdrive/My Drive/EIP3.0/model_99_0.12999999523162842.h5\")\n","\n","from keras import optimizers\n","\n","clr_triangular = CyclicLR(mode='triangular')\n","\n","epochs = 50\n","learning_rate = 0.01\n","decay_rate = learning_rate / epochs\n","momentum = 0.9\n","# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n","sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy',\n","             optimizer=sgd,\n","             metrics=['accuracy'])\n","epochs = 100\n","for epoch in range(epochs):  \n","  print(\"150 to 250 epochs\")\n","  model.fit_generator(train_generator,epochs=1, callbacks=[clr_triangular], samples_per_epoch = 100000, verbose=1)\n","  if epoch % 10 == 0:\n","    results = model.evaluate_generator(validation_generator, steps=1) \n","    print('Accuracy :', (results[1]*100.0))\n","\n","\n","model.save_weights(\"/content/gdrive/My Drive/EIP3.0/model_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n","print(\"Saved model to disk\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 100000 images belonging to 200 classes.\n","Found 10000 images belonging to 200 classes.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-30b695169185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                     color_mode='rgb', class_mode='categorical', batch_size=200, shuffle=True, seed=42)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResnetBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_resnet_50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#load weights to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# model.load_weights(\"/content/gdrive/My Drive/EIP3.0/model_99_0.12999999523162842.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-8893df121864>\u001b[0m in \u001b[0;36mbuild_resnet_50\u001b[0;34m(input_shape, num_outputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_resnet_50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mResnetBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResnetBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_resnet_50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-8893df121864>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(input_shape, num_outputs, block_fn, repetitions)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepetitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_residual_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_first_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mfilters\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-8893df121864>\u001b[0m in \u001b[0;36mf\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0minit_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             input = block_function(filters=filters, init_strides=init_strides,\n\u001b[0;32m--> 104\u001b[0;31m                                    is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-8893df121864>\u001b[0m in \u001b[0;36mf\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mconv_3_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bn_relu_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bn_relu_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_3_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_shortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-8893df121864>\u001b[0m in \u001b[0;36m_shortcut\u001b[0;34m(input, residual)\u001b[0m\n\u001b[1;32m     90\u001b[0m                           kernel_regularizer=l2(0.0001))(input)\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid number of arguments"]}]},{"metadata":{"id":"RL1TS1JtOS_n","colab_type":"code","outputId":"fe0dacaa-5de1-45f8-f16e-b4edac2d76db","executionInfo":{"status":"ok","timestamp":1553948704562,"user_tz":-330,"elapsed":2025,"user":{"displayName":"Naveen Bharadwaj","photoUrl":"https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg","userId":"14707737005437864614"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"cell_type":"code","source":["Avisualize_image('tiny-imagenet-200/train/n01443537/images/n01443537_10.JPEG')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['add']\n","`%matplotlib` prevents importing * from pylab and numpy\n","  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"hwMn3zmjPi8X","colab_type":"code","colab":{}},"cell_type":"code","source":["!@optimize for a wider network --> more number of channels/kernels\n","\n","32 -> 64 -> 128 -> 256 -> 512 \n","64 -> 128 -> 256 -> 512 -> 1024\n","128 -> 256 -> 512 -> 1024 -> 2048\n","\n","\n","32 x 32 x 3\n","30 x 30 x 3 x 32\n","28 x 28 x 3 x 64\n","26 x 26 x 3 x 128\n","24 x 24 x 3 x 256\n","22 x 22 x 3 x 512\n","\n","\n","occlusion - image augmentation\n","https://github.com/mdbloice/Augmentor\n","https://medium.com/@arindambaidya168/https-medium-com-arindambaidya168-using-keras-imagedatagenerator-b94a87cdefad\n","  \n","  \n","rewrite the model \n","gradient descent with momentum\n","\n","validate every 10 epochs\n","re-write the data generator\n","\n","first 50 epochs with 16x16\n","next 100 with 32x32\n","next 100 with 64x64\n","model checkpoint and restore with varying input sizes"],"execution_count":0,"outputs":[]}]}