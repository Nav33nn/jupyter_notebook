{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2050574,
     "output_embedded_package_id": "1JoOX8CEnrdcbH1qVuTGpxF7tljO63hkR"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59901,
     "status": "ok",
     "timestamp": 1554716570454,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "iB_Xb9Ha6ZOA",
    "outputId": "2b3e74cc-14be-430b-e351-9d8cb95e63a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_data():\n",
    "  #download the dataset\n",
    "  !wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "  #unzip the dataset\n",
    "  !unzip tiny-imagenet-200.zip\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1425,
     "status": "ok",
     "timestamp": 1554717041149,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "kGzPPGJIp0YM",
    "outputId": "a6a897a3-41a9-4261-93c8-716c89118bcf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val_0.JPEG</td>\n",
       "      <td>n03444034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val_1.JPEG</td>\n",
       "      <td>n04067472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val_2.JPEG</td>\n",
       "      <td>n04070727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val_3.JPEG</td>\n",
       "      <td>n02808440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val_4.JPEG</td>\n",
       "      <td>n02808440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         File      Class\n",
       "0  val_0.JPEG  n03444034\n",
       "1  val_1.JPEG  n04067472\n",
       "2  val_2.JPEG  n04070727\n",
       "3  val_3.JPEG  n02808440\n",
       "4  val_4.JPEG  n02808440"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare val data classes to file relation\n",
    "import pandas as pd\n",
    "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21616,
     "status": "ok",
     "timestamp": 1554717551104,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "yCaCSnPKpLCJ",
    "outputId": "710d4396-16b9-4652-ec0b-ff34ce27ad64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (3.4.5.20)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (1.6.4.post2)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
      "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.9.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image) (4.4.0)\n",
      "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.8)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.13.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.4.5.20)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug) (4.1.1)\n",
      "Collecting numpy>=1.15.0 (from imgaug)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 2.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.1.0)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.6.4.post2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug) (3.0.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.11.0)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug) (2.4.1)\n",
      "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.2)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.0.2)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->imgaug) (0.46)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug) (2.3.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (40.9.0)\n",
      "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy\n",
      "  Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "Successfully installed numpy-1.16.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:334: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "!pip install six numpy scipy Pillow matplotlib scikit-image opencv-python imageio Shapely\n",
    "!pip install imgaug\n",
    "\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "aug1 = iaa.Sequential([iaa.GaussianBlur(sigma=(0, 2.0)),\n",
    "                      iaa.Dropout(0.02, name=\"Dropout\"),\n",
    "                      iaa.AdditiveGaussianNoise(scale=0.01*255, name=\"MyLittleNoise\"),\n",
    "                      iaa.AdditiveGaussianNoise(loc=32, scale=0.0001*255, name=\"SomeOtherNoise\"),\n",
    "                      iaa.Affine(translate_px={\"x\": (-40, 40)}, name=\"Affine\")\n",
    "                      ])\n",
    "\n",
    "def other_augmentations(image):\n",
    "  image = aug1.augment_image(image)\n",
    "  return image\n",
    "\n",
    "\n",
    "# Use Augmentaion parameters as required.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale= 1./255,\n",
    "    zoom_range = 0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rotation_range=120,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zca_whitening=True, \n",
    "    zca_epsilon=1e-06,\n",
    "    fill_mode = \"reflect\",\n",
    "    preprocessing_function=other_augmentations\n",
    "    )\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8075,
     "status": "ok",
     "timestamp": 1554660408396,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "h9EfkYPrqEtS",
    "outputId": "ee8a9c5c-d865-4f07-e1f3-06d3e5636654"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "#initialize train and val data generator\n",
    "train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', target_size=(64, 64), color_mode='rgb', \n",
    "                                                    batch_size=500, class_mode='categorical', shuffle=True, seed=42)\n",
    "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', x_col='File', y_col='Class', target_size=(64,64),\n",
    "                                                    color_mode='rgb', class_mode='categorical', batch_size=500, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6412,
     "status": "ok",
     "timestamp": 1554660411386,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "OaP26SouSfqn",
    "outputId": "782f34fd-f92d-4cfe-842e-1909a8867d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 16, 16, 256)  16640       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 256)  0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 256)  0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 16, 16, 512)  131584      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 16, 16, 512)  33280       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 16, 16, 512)  2048        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 512)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 512)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 256)  0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 16, 16, 256)  0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 16, 16, 512)  131584      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 512)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 16, 16, 512)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 16, 16, 256)  0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 16, 16, 256)  0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 16, 16, 512)  131584      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 16, 16, 512)  2048        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 512)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 512)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 8, 8, 512)    262656      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 512)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 512)    0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 8, 8, 1024)   525312      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 8, 8, 1024)   525312      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 8, 8, 1024)   4096        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 1024)   0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 512)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 512)    0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 8, 8, 1024)   525312      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 1024)   0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 8, 8, 512)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 8, 8, 512)    0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 8, 8, 1024)   525312      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 1024)   0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 8, 8, 1024)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 8, 8, 512)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 8, 8, 512)    0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 8, 8, 1024)   525312      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 8, 8, 1024)   4096        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 1024)   0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 8, 8, 1024)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "final_layer (Conv2D)            (None, 8, 8, 200)    205000      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 200)          0           final_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 200)          0           avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 16,646,344\n",
      "Trainable params: 16,620,616\n",
      "Non-trainable params: 25,728\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  # for batch normalization layer, we assume\n",
    "  # the input data is in channel last format\n",
    "  bn_axis = 3\n",
    "\n",
    "  filters1, filters2, filters3 = filters\n",
    "\n",
    "  # main path, note that setting the kernel_initializer seed here is only used\n",
    "  # for reproducibility, we techniqually don't need it\n",
    "  x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=(1, 1),\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
    "  x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='same', name=conv_name_base + '2b')(x)\n",
    "  x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "  \n",
    "  x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='valid', name=conv_name_base + '2c')(x)\n",
    "  x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "  # this line is the core component of resnet, the skip connection, i.e.\n",
    "  # having a shortcut to the main path before the activation, when addition\n",
    "  # is performed on convolutional layers, the element-wise addition is performed\n",
    "  # on their feature maps, i.e. channel by channel\n",
    "  x = layers.add([x, input_tensor])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x\n",
    "  \n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "  # for batch normalization layer, we assume\n",
    "  # the input data is in channel last format,\n",
    "  # which is the case if we are using the default\n",
    "  # keras' backend tensorflow\n",
    "  bn_axis = 3\n",
    "\n",
    "  filters1, filters2, filters3 = filters\n",
    "\n",
    "  # main path, note that setting the kernel_initializer set here is only used\n",
    "  # for reproducibility, we techniqually don't need it\n",
    "  x = layers.Conv2D(filters1, kernel_size=(1, 1), strides=strides,\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='valid', name=conv_name_base + '2a')(input_tensor)\n",
    "  x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  x = layers.Conv2D(filters2, kernel_size, strides=(1, 1),\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='same', name=conv_name_base + '2b')(x)\n",
    "  x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "  x = layers.Activation('relu')(x)\n",
    "\n",
    "  \n",
    "  x = layers.Conv2D(filters3, kernel_size=(1, 1), strides=(1, 1),\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='valid', name=conv_name_base + '2c')(x)\n",
    "  x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "  # we resize the input so its dimension will match the output dimension\n",
    "  # of the main path\n",
    "  shortcut = layers.Conv2D(filters3, kernel_size=(1, 1), strides=strides,\n",
    "                           kernel_initializer=glorot_uniform(seed=0),\n",
    "                           padding='valid', name=conv_name_base + '1')(input_tensor)\n",
    "  shortcut = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut) \n",
    "\n",
    "  # this line is the core component of resnet, the skip connection, i.e.\n",
    "  # having a shortcut to the main path before the activation\n",
    "  x = layers.add([x, shortcut])\n",
    "  x = layers.Activation('relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "def ResNet(input_shape, n_classes):\n",
    "    \"\"\"\n",
    "    Definition of ResNet\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n",
    "    \"\"\"\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    bn_axis = 3\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(64, (7, 7),\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name='conv1')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 512], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [256, 256, 512], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 512], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 1024], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 1024], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 1024], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [512, 512, 1024], stage=3, block='d')\n",
    "#     x = identity_block(x, 3, [128, 128, 512, 1024], stage=3, block='d')\n",
    "    \n",
    "#     x = conv_block(x, 3, [1024, 1024, 2048], stage=4, block='a')\n",
    "#     x = identity_block(x, 3, [1024, 1024, 2048], stage=4, block='b')\n",
    "#     x = identity_block(x, 3, [1024, 1024, 2048], stage=4, block='c')\n",
    "  \n",
    "    x = layers.Conv2D(n_classes, (1,1), strides=(1, 1),\n",
    "                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                    padding='same', name='final_layer')(x)\n",
    "    img_output = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "#     img_output = layers.Dense(n_classes, activation='softmax', name='fc' + str(n_classes))(x)\n",
    "    img_output = Activation('softmax')(img_output)\n",
    "    model = Model(inputs=img_input, outputs=img_output, name='resnet')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = ResNet((64, 64, 3), 200)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vxIBpxX7D-OK"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.callbacks import *\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23262,
     "status": "ok",
     "timestamp": 1554527838555,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "JGnnupiIcQzX",
    "outputId": "590063a0-f2d8-4927-beac-bfeefebf102c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "f9hN1mJscQwf"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "\n",
    "epochs = 80\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = optimizers.RMSprop(lr=1e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3199
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29407050,
     "status": "ok",
     "timestamp": 1554515825099,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "wHA7a1XfcQuC",
    "outputId": "e0a2230c-504e-4429-d945-18a0cccfd862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=1, callbacks=[], verbose=1, steps_per_epoch=200)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:718: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 407s 2s/step - loss: 4.7170 - acc: 0.0649\n",
      "epoch 0 of 80\n",
      "Accuracy : 8.799999952316284\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 4.2405 - acc: 0.1161\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 389s 2s/step - loss: 3.9985 - acc: 0.1466\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 389s 2s/step - loss: 3.8256 - acc: 0.1714\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 389s 2s/step - loss: 3.6820 - acc: 0.1941\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 389s 2s/step - loss: 3.5452 - acc: 0.2176\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 3.4425 - acc: 0.2342\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 3.3395 - acc: 0.2506\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 3.2568 - acc: 0.2651\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 3.1780 - acc: 0.2785\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 392s 2s/step - loss: 3.1058 - acc: 0.2924\n",
      "epoch 10 of 80\n",
      "Accuracy : 26.399999856948853\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 3.0282 - acc: 0.3051\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.9700 - acc: 0.3152\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.9167 - acc: 0.3240\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.8463 - acc: 0.3369\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 2.7995 - acc: 0.3468\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 394s 2s/step - loss: 2.7491 - acc: 0.3547\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 2.7001 - acc: 0.3652\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.6474 - acc: 0.3741\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.5988 - acc: 0.3828\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.5573 - acc: 0.3927\n",
      "epoch 20 of 80\n",
      "Accuracy : 35.600000619888306\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.5139 - acc: 0.3993\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.4702 - acc: 0.4065\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.4271 - acc: 0.4171\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 389s 2s/step - loss: 2.3974 - acc: 0.4223\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.3625 - acc: 0.4292\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.3139 - acc: 0.4395\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.2765 - acc: 0.4467\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.2456 - acc: 0.4507\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.2076 - acc: 0.4596\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.1761 - acc: 0.4656\n",
      "epoch 30 of 80\n",
      "Accuracy : 36.19999885559082\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.1390 - acc: 0.4713\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.1021 - acc: 0.4802\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.0668 - acc: 0.4873\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.0312 - acc: 0.4950\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 2.0006 - acc: 0.5013\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.9661 - acc: 0.5086\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.9284 - acc: 0.5166\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.8901 - acc: 0.5240\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 391s 2s/step - loss: 1.8636 - acc: 0.5319\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.8252 - acc: 0.5388\n",
      "epoch 40 of 80\n",
      "Accuracy : 39.399999380111694\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.8040 - acc: 0.5434\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.7618 - acc: 0.5520\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.7342 - acc: 0.5564\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.7029 - acc: 0.5648\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 390s 2s/step - loss: 1.6726 - acc: 0.5722\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 394s 2s/step - loss: 1.6402 - acc: 0.5795\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.6027 - acc: 0.5876\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.5735 - acc: 0.5923\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.5403 - acc: 0.6015\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.5162 - acc: 0.6065\n",
      "epoch 50 of 80\n",
      "Accuracy : 37.79999911785126\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.4882 - acc: 0.6129\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.4560 - acc: 0.6184\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 394s 2s/step - loss: 1.4295 - acc: 0.6255\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.3984 - acc: 0.6330\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 394s 2s/step - loss: 1.3661 - acc: 0.6419\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.3335 - acc: 0.6487\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.3025 - acc: 0.6566\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.2702 - acc: 0.6631\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.2484 - acc: 0.6679\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.2158 - acc: 0.6754\n",
      "epoch 60 of 80\n",
      "Accuracy : 36.000001430511475\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.1899 - acc: 0.6839\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.1619 - acc: 0.6903\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.1285 - acc: 0.6966\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.1085 - acc: 0.7023\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.0834 - acc: 0.7069\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.0600 - acc: 0.7147\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 1.0340 - acc: 0.7213\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 394s 2s/step - loss: 1.0112 - acc: 0.7262\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 0.9831 - acc: 0.7327\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 0.9558 - acc: 0.7404\n",
      "epoch 70 of 80\n",
      "Accuracy : 39.59999978542328\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 0.9372 - acc: 0.7444\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 395s 2s/step - loss: 0.9094 - acc: 0.7512\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 0.8861 - acc: 0.7574\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 0.8582 - acc: 0.7648\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 0.8355 - acc: 0.7709\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 0.8126 - acc: 0.7756\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 0.7963 - acc: 0.7806\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 393s 2s/step - loss: 0.7716 - acc: 0.7858\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 392s 2s/step - loss: 0.7583 - acc: 0.7898\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):  \n",
    "  model.fit_generator(train_generator,epochs=1, callbacks=[], samples_per_epoch = 100000, verbose=1)\n",
    "  if epoch % 10 == 0:\n",
    "    results = model.evaluate_generator(validation_generator, steps=1) \n",
    "    print(\"epoch \"+str(epoch)+\" of \"+str(epochs))\n",
    "    print('Accuracy :', (results[1]*100.0))\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/My Drive/Colab Notebooks/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1993,
     "status": "ok",
     "timestamp": 1554519366478,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "UJR2adpjsp_F",
    "outputId": "23d24d9f-9ab7-4cfd-9726-cca6883b597a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79\n",
      "Accuracy : 39.19999897480011\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate_generator(validation_generator, steps=1) \n",
    "print(\"epoch \"+str(epoch))\n",
    "print('Accuracy :', (results[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Yom-HhxGPm02"
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, model_json_path):\n",
    "  from keras.models import Model\n",
    "  from keras.models import model_from_json\n",
    "  json_file = open(model_json_path, 'r')\n",
    "  loaded_model_json = json_file.read()\n",
    "  json_file.close()\n",
    "  loaded_model = model_from_json(loaded_model_json)\n",
    "  # load weights into new model\n",
    "  loaded_model.load_weights(model_path)\n",
    "  print(\"Loaded model from disk\")\n",
    "  return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gprYoGl0r88J"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_79_0.3959999978542328.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HLrRoyGGd-oP"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "\n",
    "epochs = 160\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = optimizers.RMSprop(lr=1e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3216
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32335817,
     "status": "ok",
     "timestamp": 1554560192763,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "MUItyOX6cQkX",
    "outputId": "c4075650-f582-470b-d486-18a767039ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 80 to 160\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=1, callbacks=[<__main__..., verbose=1, steps_per_epoch=200)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:718: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 418s 2s/step - loss: 4.4549 - acc: 0.1134\n",
      "epoch 80\n",
      "Accuracy : 15.000000596046448\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.7978 - acc: 0.1742\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.5865 - acc: 0.2102\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.4762 - acc: 0.2319\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 3.3846 - acc: 0.2503\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 3.2613 - acc: 0.2695\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 3.0493 - acc: 0.2990\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.8957 - acc: 0.3276\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.7764 - acc: 0.3486\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.6782 - acc: 0.3674\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.5352 - acc: 0.3962\n",
      "epoch 90\n",
      "Accuracy : 37.99999952316284\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.3636 - acc: 0.4323\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.2149 - acc: 0.4582\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 2.0933 - acc: 0.4846\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.9528 - acc: 0.5158\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.8447 - acc: 0.5375\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.7254 - acc: 0.5632\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.6205 - acc: 0.5872\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.5072 - acc: 0.6099\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4070 - acc: 0.6341\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.3553 - acc: 0.6473\n",
      "epoch 100\n",
      "Accuracy : 49.59999918937683\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 1.3698 - acc: 0.6402\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.3801 - acc: 0.6373\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4040 - acc: 0.6294\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4224 - acc: 0.6264\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4543 - acc: 0.6166\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4652 - acc: 0.6126\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4827 - acc: 0.6077\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.5028 - acc: 0.6015\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.5125 - acc: 0.5997\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.4734 - acc: 0.6087\n",
      "epoch 110\n",
      "Accuracy : 46.39999866485596\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.3558 - acc: 0.6358\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.2390 - acc: 0.6649\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.1215 - acc: 0.6942\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 1.0079 - acc: 0.7220\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 0.9039 - acc: 0.7480\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 403s 2s/step - loss: 0.7926 - acc: 0.7770\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.6897 - acc: 0.8046\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5960 - acc: 0.8304\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5208 - acc: 0.8521\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.4704 - acc: 0.8659\n",
      "epoch 120\n",
      "Accuracy : 48.80000054836273\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5032 - acc: 0.8532\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5282 - acc: 0.8464\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5742 - acc: 0.8312\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.6142 - acc: 0.8194\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.6680 - acc: 0.8042\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.7123 - acc: 0.7901\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.7602 - acc: 0.7782\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.8118 - acc: 0.7630\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.8815 - acc: 0.7472\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.8452 - acc: 0.7536\n",
      "epoch 130\n",
      "Accuracy : 43.799999356269836\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.7613 - acc: 0.7772\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.6628 - acc: 0.8050\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5786 - acc: 0.8274\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.4990 - acc: 0.8517\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.4167 - acc: 0.8753\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.3481 - acc: 0.8965\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.2900 - acc: 0.9126\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.2355 - acc: 0.9297\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.1949 - acc: 0.9427\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.1704 - acc: 0.9498\n",
      "epoch 140\n",
      "Accuracy : 46.59999907016754\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.1944 - acc: 0.9420\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.2269 - acc: 0.9312\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.2614 - acc: 0.9199\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.3040 - acc: 0.9064\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.3593 - acc: 0.8907\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.4012 - acc: 0.8775\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.4514 - acc: 0.8622\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.5060 - acc: 0.8477\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.5562 - acc: 0.8334\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.5537 - acc: 0.8343\n",
      "epoch 150\n",
      "Accuracy : 43.99999976158142\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.4930 - acc: 0.8506\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.4262 - acc: 0.8697\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.3690 - acc: 0.8887\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.3045 - acc: 0.9068\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.2572 - acc: 0.9221\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.2108 - acc: 0.9350\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.1757 - acc: 0.9465\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.1408 - acc: 0.9574\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.1101 - acc: 0.9668\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print('epochs 80 to 160')\n",
    "\n",
    "for epoch in range(80,160):\n",
    "  model.fit_generator(train_generator,epochs=1, callbacks=[clr_triangular], samples_per_epoch = 100000, verbose=1)\n",
    "  if epoch % 10 == 0:\n",
    "    results = model.evaluate_generator(validation_generator, steps=1) \n",
    "    print(\"epoch \"+str(epoch))\n",
    "    print('Accuracy :', (results[1]*100.0))\n",
    "  \n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/My Drive/Colab Notebooks/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25861,
     "status": "ok",
     "timestamp": 1554623087516,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "Au4piBD4vjT-",
    "outputId": "8ff343a8-a53d-4d85-a719-78116f528ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3182
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12775526,
     "status": "ok",
     "timestamp": 1554655772589,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "DGcOw51Svk0D",
    "outputId": "c2c222bd-77b9-4a89-da6c-765e8a5d4154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 160 to 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=1, callbacks=[<__main__..., verbose=1, steps_per_epoch=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:718: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 422s 2s/step - loss: 0.1000 - acc: 0.9697\n",
      "epoch 160\n",
      "Accuracy : 45.60000002384186\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1133 - acc: 0.9652\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1395 - acc: 0.9565\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1697 - acc: 0.9478\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.2025 - acc: 0.9375\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2384 - acc: 0.9263\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2790 - acc: 0.9148\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.3242 - acc: 0.8997\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.3706 - acc: 0.8880\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.4094 - acc: 0.8770\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.4169 - acc: 0.8743\n",
      "epoch 170\n",
      "Accuracy : 41.80000126361847\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.3647 - acc: 0.8887\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.3088 - acc: 0.9052\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.2722 - acc: 0.9170\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2294 - acc: 0.9289\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1919 - acc: 0.9407\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1571 - acc: 0.9520\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.1253 - acc: 0.9617\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1019 - acc: 0.9685\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.0803 - acc: 0.9756\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.0710 - acc: 0.9783\n",
      "epoch 180\n",
      "Accuracy : 46.59999907016754\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0818 - acc: 0.9755\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0981 - acc: 0.9700\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1212 - acc: 0.9619\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1490 - acc: 0.9536\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1769 - acc: 0.9450\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2062 - acc: 0.9367\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2429 - acc: 0.9251\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2812 - acc: 0.9139\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.3224 - acc: 0.9012\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.3209 - acc: 0.9021\n",
      "epoch 190\n",
      "Accuracy : 45.19999921321869\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2877 - acc: 0.9126\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2436 - acc: 0.9254\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2116 - acc: 0.9343\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1769 - acc: 0.9448\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1493 - acc: 0.9538\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1221 - acc: 0.9615\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0977 - acc: 0.9703\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0802 - acc: 0.9754\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0638 - acc: 0.9805\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0543 - acc: 0.9834\n",
      "epoch 200\n",
      "Accuracy : 45.19999921321869\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0639 - acc: 0.9807\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0778 - acc: 0.9761\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0939 - acc: 0.9706\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1158 - acc: 0.9641\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1413 - acc: 0.9557\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1706 - acc: 0.9476\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1972 - acc: 0.9398\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.2289 - acc: 0.9284\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2649 - acc: 0.9189\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2682 - acc: 0.9181\n",
      "epoch 210\n",
      "Accuracy : 43.00000071525574\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.2366 - acc: 0.9269\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.2052 - acc: 0.9365\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1770 - acc: 0.9454\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1517 - acc: 0.9533\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.1248 - acc: 0.9613\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1021 - acc: 0.9687\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0800 - acc: 0.9747\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0659 - acc: 0.9794\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0519 - acc: 0.9838\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0448 - acc: 0.9864\n",
      "epoch 220\n",
      "Accuracy : 48.60000014305115\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0522 - acc: 0.9839\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0669 - acc: 0.9794\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0778 - acc: 0.9761\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0990 - acc: 0.9688\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1229 - acc: 0.9613\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.1426 - acc: 0.9552\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.1683 - acc: 0.9484\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.1971 - acc: 0.9388\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.2286 - acc: 0.9298\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.2300 - acc: 0.9298\n",
      "epoch 230\n",
      "Accuracy : 43.39999854564667\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.2094 - acc: 0.9363\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.1794 - acc: 0.9449\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.1533 - acc: 0.9523\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.1281 - acc: 0.9606\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.1089 - acc: 0.9664\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0913 - acc: 0.9712\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0711 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0566 - acc: 0.9824\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0474 - acc: 0.9850\n",
      "epoch 239\n",
      "Accuracy : 45.39999961853027\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_159_0.4399999976158142.h5\")\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "\n",
    "epochs = 160\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = optimizers.RMSprop(lr=1e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print('epochs 160 to 240')\n",
    "\n",
    "for epoch in range(160,240):\n",
    "  model.fit_generator(train_generator,epochs=1, callbacks=[clr_triangular], samples_per_epoch = 100000, verbose=1)\n",
    "  if epoch % 10 == 0:\n",
    "    results = model.evaluate_generator(validation_generator, steps=1) \n",
    "    print(\"epoch \"+str(epoch))\n",
    "    print('Accuracy :', (results[1]*100.0))\n",
    "  \n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/My Drive/Colab Notebooks/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "results = model.evaluate_generator(validation_generator, steps=1) \n",
    "print(\"epoch \"+str(epoch))\n",
    "print('Accuracy :', (results[1]*100.0))\n",
    "\n",
    "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23390,
     "status": "ok",
     "timestamp": 1554656869920,
     "user": {
      "displayName": "Naveen Bharadwaj",
      "photoUrl": "https://lh4.googleusercontent.com/-yYj6SAmNV4o/AAAAAAAAAAI/AAAAAAAAAE0/TU_U_ezgx44/s64/photo.jpg",
      "userId": "14707737005437864614"
     },
     "user_tz": -330
    },
    "id": "qoXh1BoUvpuk",
    "outputId": "13695478-9cb8-44c8-86b2-2a9496d18cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "OpRfqsQ2vpHt",
    "outputId": "a6c208a6-8e5e-49b0-ba41-0b5dbe98d8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 240 to 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=1, callbacks=[<__main__..., validation_data=<keras_pre..., validation_steps=10000, verbose=1, steps_per_epoch=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:699: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:718: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 7413s 37s/step - loss: 0.0457 - acc: 0.9860 - val_loss: 4.6235 - val_acc: 0.4564\n",
      "epoch 240\n",
      "Accuracy : 43.799999356269836\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 7393s 37s/step - loss: 0.0478 - acc: 0.9852 - val_loss: 4.6675 - val_acc: 0.4515\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 7402s 37s/step - loss: 0.0552 - acc: 0.9826 - val_loss: 4.7611 - val_acc: 0.4513\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 7376s 37s/step - loss: 0.0702 - acc: 0.9782 - val_loss: 4.7605 - val_acc: 0.4407\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 7417s 37s/step - loss: 0.0834 - acc: 0.9740 - val_loss: 4.8524 - val_acc: 0.4359\n",
      "Epoch 1/1\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1080 - acc: 0.9671"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_239_0.45399999618530273.h5\")\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "\n",
    "epochs = 160\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "# sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "opt = optimizers.RMSprop(lr=1e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=opt,\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "print('epochs 240 to 300')\n",
    "\n",
    "for epoch in range(240,300):\n",
    "  model.fit_generator(train_generator,epochs=1, callbacks=[clr_triangular], samples_per_epoch = 100000, validation_data = validation_generator , validation_steps = 10000,verbose=1)\n",
    "  if epoch % 10 == 0:\n",
    "    results = model.evaluate_generator(validation_generator, steps=1) \n",
    "    print(\"epoch \"+str(epoch))\n",
    "    print('Accuracy :', (results[1]*100.0))\n",
    "  \n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"/content/gdrive/My Drive/Colab Notebooks/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "results = model.evaluate_generator(validation_generator, steps=1) \n",
    "print(\"epoch \"+str(epoch))\n",
    "print('Accuracy :', (results[1]*100.0))\n",
    "\n",
    "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/model_5mar_custom_64x64_\"+str(epoch)+\"_\"+str(results[1])+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9CRTB7kjMq1"
   },
   "source": [
    "On the execution block above you can see \n",
    "\n",
    "200/200 [==============================] - 403s 2s/step - loss: 1.5072 - acc: 0.6099\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 403s 2s/step - loss: 1.4070 - acc: 0.6341\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 404s 2s/step - loss: 1.3553 - acc: 0.6473\n",
    "\n",
    "epoch 100\n",
    "\n",
    "Accuracy : 49.59999918937683\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 404s 2s/step - loss: 1.3698 - acc: 0.6402\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 403s 2s/step - loss: 1.3801 - acc: 0.6373\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 403s 2s/step - loss: 1.4040 - acc: 0.6294\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 403s 2s/step - loss: 1.4224 - acc: 0.6264\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "200/200 [==============================] - 403s 2s/step - loss: 1.4543 - acc: 0.6166\n",
    "\n",
    "\n",
    "This shows that the validation accuracy was at 49.6% at the end of 100th epoch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4.ipynb",
   "provenance": [
    {
     "file_id": "1JnhubNGmuEIFPFh9usQQDTZn9BvS6rnb",
     "timestamp": 1554394176771
    },
    {
     "file_id": "1ym5VJtCIxjgtOxxN0ao_R2qNxTU0YVvy",
     "timestamp": 1554184744077
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
